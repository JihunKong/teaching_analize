#!/usr/bin/env python3
"""
AIBOA Analysis Service
Multiple framework analysis using OpenAI GPT-4o-mini API
"""

import os
import uuid
import logging
from datetime import datetime
from typing import Dict, Any, Optional, List

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, Response
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, validator
import redis
import json
import requests

# Import report generators
from html_report_generator import HTMLReportGenerator
from pdf_report_generator import PDFReportGenerator, is_pdf_generation_available
from diagnostic_report_generator import DiagnosticReportGenerator

# Import Module 4 advanced generators
from advanced_pdf_generator import AdvancedPDFGenerator
from visualization import Matrix3DVisualizer
from exporters import ExcelReportExporter

# Import database
from database import (
    get_db, store_analysis, update_framework_usage, get_research_statistics,
    init_database, AnalysisResultDB
)
from sqlalchemy.orm import Session

# Import Module 3 evaluation components
from core.evaluation_service import EvaluationService
from core.cbil_integration import CBILIntegration

# Import semantic cache for consistency guarantee
from utils.semantic_cache import SemanticCache

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="AIBOA Analysis Service",
    description="Multiple framework analysis for educational discourse",
    version="2.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount static files for design system CSS
static_dir = os.path.join(os.path.dirname(__file__), "static")
if os.path.exists(static_dir):
    app.mount("/static", StaticFiles(directory=static_dir), name="static")
    logger.info(f"âœ“ Static files mounted: {static_dir}")
else:
    logger.warning(f"âš ï¸  Static directory not found: {static_dir}")

# Redis connection
redis_client = redis.Redis(
    host=os.getenv('REDIS_HOST', 'redis'),
    port=int(os.getenv('REDIS_PORT', 6379)),
    password=os.getenv('REDIS_PASSWORD'),
    decode_responses=True
)

# Initialize Semantic Cache for consistency guarantee
# Caches first LLM classification result to ensure 100% reproducibility
semantic_cache = SemanticCache(redis_client)
logger.info("âœ“ Semantic Cache initialized for guaranteed consistency")

# OpenAI API configuration
# Read Upstage configuration
UPSTAGE_BASE_URL = os.getenv('UPSTAGE_BASE_URL', 'https://api.upstage.ai/v1')
GPT_MODEL = os.getenv('GPT_MODEL', 'solar-pro2')
UPSTAGE_API_KEY = os.getenv('UPSTAGE_API_KEY')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# Determine which key to use based on the base URL or preference
# If using Upstage, we must use UPSTAGE_API_KEY
API_KEY = UPSTAGE_API_KEY if UPSTAGE_API_KEY else OPENAI_API_KEY

if not API_KEY:
    logger.warning("No API Key (UPSTAGE_API_KEY or OPENAI_API_KEY) found in environment variables")

# Import OpenAI
from openai import OpenAI
openai_client = OpenAI(api_key=API_KEY, base_url=UPSTAGE_BASE_URL) if API_KEY else None

class AnalysisRequest(BaseModel):
    text: str
    framework: str = "cbil"  # cbil, student_discussion, lesson_coaching, etc.
    metadata: Optional[Dict[str, Any]] = {}
    segments: Optional[List[Dict[str, Any]]] = []  # Segments from Module 1 with timestamps

class AnalysisFramework(BaseModel):
    id: str
    name: str
    description: str
    prompt_template: str

# Analysis Framework Definitions
ANALYSIS_FRAMEWORKS = {
    "cbil": {
        "name": "ê°œë…ê¸°ë°˜ íƒêµ¬ ìˆ˜ì—…(CBIL) ë¶„ì„",
        "description": "7ë‹¨ê³„ CBIL ë¶„ì„ ë° í‰ê°€",
        "prompt": """**CBIL 7ë‹¨ê³„ ë¶„ì„ - ì ìˆ˜ ì¶œë ¥ í•„ìˆ˜**

ğŸ“‹ **ì¤‘ìš” ì§€ì‹œì‚¬í•­**
- ê° ë‹¨ê³„ë§ˆë‹¤ ë°˜ë“œì‹œ "ì ìˆ˜: Xì " í˜•ì‹ìœ¼ë¡œ ì ìˆ˜ë¥¼ ëª…ì‹œí•  ê²ƒ
- 0ì (ì—†ìŒ), 1ì (ë¶€ì¡±), 2ì (ë³´í†µ), 3ì (ìš°ìˆ˜) ì¤‘ í•˜ë‚˜ë¡œ ì±„ì 
- 7ë‹¨ê³„ ëª¨ë‘ ë¶„ì„í•˜ê³  ì ìˆ˜ ë¶€ì—¬ í•„ìˆ˜

ğŸ¯ **ë¶„ì„ ëª©ì **
ê°œë…ê¸°ë°˜ íƒêµ¬í•™ìŠµ(CBIL) 7ë‹¨ê³„ ì‹¤í–‰ í‰ê°€ ë° ì ìˆ˜ ë¶€ì—¬ (0~3ì )

ğŸ“ **í•„ìˆ˜ ì¶œë ¥ í˜•ì‹** (ì •í™•íˆ ì´ í˜•ì‹ì„ ë”°ë¥¼ ê²ƒ):

#### 1. Engage (í¥ë¯¸ ìœ ë„ ë° ì—°ê²°)
[ìˆ˜ì—… ì¥ë©´ ë¶„ì„ ë‚´ìš©...]
**ì ìˆ˜: Xì **

#### 2. Focus (íƒêµ¬ ë°©í–¥ ì„¤ì •)  
[ìˆ˜ì—… ì¥ë©´ ë¶„ì„ ë‚´ìš©...]
**ì ìˆ˜: Xì **

#### 3. Investigate (ìë£Œ íƒìƒ‰ ë° ê°œë… í˜•ì„±)
[ìˆ˜ì—… ì¥ë©´ ë¶„ì„ ë‚´ìš©...]
**ì ìˆ˜: Xì **

#### 4. Organize (ì¦ê±° ì¡°ì§í™”)
[ìˆ˜ì—… ì¥ë©´ ë¶„ì„ ë‚´ìš©...]
**ì ìˆ˜: Xì **

#### 5. Generalize (ì¼ë°˜í™”)
[ìˆ˜ì—… ì¥ë©´ ë¶„ì„ ë‚´ìš©...]
**ì ìˆ˜: Xì **

#### 6. Transfer (ì „ì´)
[ìˆ˜ì—… ì¥ë©´ ë¶„ì„ ë‚´ìš©...]
**ì ìˆ˜: Xì **

#### 7. Reflect (ì„±ì°°)
[ìˆ˜ì—… ì¥ë©´ ë¶„ì„ ë‚´ìš©...]
**ì ìˆ˜: Xì **

ğŸ” **ì ìˆ˜ ê¸°ì¤€**
- 3ì : ê°œë… ì¤‘ì‹¬ì˜ íƒêµ¬ í™œë™ì´ ëª…í™•íˆ êµ¬í˜„ë¨
- 2ì : ë¶€ë¶„ì ìœ¼ë¡œ ê°œë… ì¤‘ì‹¬ ìš”ì†Œê°€ ë‚˜íƒ€ë‚¨  
- 1ì : ì§€ì‹ ì¤‘ì‹¬ì´ì§€ë§Œ ê°œë… ì¤‘ì‹¬ ìš”ì†Œ ì¼ë¶€ ì¡´ì¬
- 0ì : í•´ë‹¹ ë‹¨ê³„ê°€ ë‚˜íƒ€ë‚˜ì§€ ì•ŠìŒ

âš ï¸ **ì£¼ì˜**: ë°˜ë“œì‹œ ìœ„ í˜•ì‹ì„ ë”°ë¼ 7ë‹¨ê³„ ëª¨ë‘ ë¶„ì„í•˜ê³  ê°ê° ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì‹œì˜¤.

**ë¶„ì„í•  í…ìŠ¤íŠ¸:**
{text}
"""
    },
    
    "student_discussion": {
        "name": "í•™ìƒì£¼ë„ ì§ˆë¬¸ê³¼ ëŒ€í™” ë° í† ë¡  ìˆ˜ì—…(í•­ìƒì„±)",
        "description": "í•™ìƒ ì£¼ë„ í† ë¡  ë° ì§ˆë¬¸ ë¶„ì„",
        "prompt": """ğŸ“‹ ë¶„ì„ ì§€ì¹¨
ë¶„ì„ ê·¼ê±°: VIDEO_TRANSCRIPTì˜ êµì‚¬Â·í•™ìƒ ì‹¤ì œ ë°œí™”/í–‰ë™ë§Œ ì‚¬ìš©(ì¶”ì •Â·ê°€ê³µ ê¸ˆì§€).

ì¬í˜„ì„±: ë™ì¼ ëŒ€í™” ì…ë ¥ ì‹œ ê²°ê³¼(ë¹ˆë„Â·ë¶„ì„ë¬¸) í•­ìƒ ë™ì¼, ëŒ€í™” ë³€ê²½ ì‹œ ê²°ê³¼ë„ ë³€ê²½.

ì¶œë ¥ ìˆœì„œ ê·œì¹™

ì§ˆë¬¸ ìœ í˜•: ì‚¬ì‹¤ì  â†’ í•´ì„ì  â†’ í‰ê°€ì  (ê³ ì •)

í›„ì† ì§ˆë¬¸ ìœ í˜•: ëª…ë£Œí™” â†’ ì´ˆì í™” â†’ ì •êµí™” â†’ í™•ì¥í™” â†’ ì…ì¦í™” (ê³ ì •)

ìˆ˜ì—…ëŒ€í™” ìœ í˜•: ë¶„ì„ë¬¸Â·ê·¸ë˜í”„ ëª¨ë‘ ë¹ˆë„ ë†’ì€ ìˆœ ì •ë ¬
(ë™ë¥  ì‹œ: ì¶”ê°€í•˜ê¸° â†’ ì°¸ì—¬í•˜ê¸° â†’ ë°˜ì‘í•˜ê¸° â†’ ìœ ë³´í•˜ê¸° â†’ ìˆ˜ìš©í•˜ê¸° â†’ ë°˜ëŒ€í•˜ê¸° â†’ ë³€í™˜í•˜ê¸°)

ë¶„ì„ ë¬¸ë‹¨: ê° ìœ í˜•ë‹¹ í•œ ë‹¨ë½(ê°œë… ì„¤ëª… + ì‹¤ì œ ë°œí™”/í–‰ë™ ì‚¬ë¡€ + ë¹ˆë„Â·ê²½í–¥ì„±)

ì‚¬ë¡€ëŠ” ë¹ˆë„ ë†’ì€ ìœ í˜•ë¶€í„° ì œì‹œ

ë¹ˆë„ í‘œê¸°: êµ¬ê°„ëª… + íšŒìˆ˜ ë²”ìœ„ (ì˜ˆ: "ì•½ê°„ ìì£¼(5~6íšŒ) ê´€ì°°ë¨")
ì§ˆì˜ì‘ë‹µ êµ¬ì¡°ì˜ ìˆ˜ì—…ì‚¬ë¡€ë¥¼ ì‹¤ì œ ìˆ˜ì—…ëŒ€í™” ì‚¬ë¡€ë¡œ ì œì‹œ

ğŸ“Š 7ë‹¨ê³„ ë¹ˆë„ êµ¬ê°„
0 = ì „í˜€(0íšŒ)
1 = ë§¤ìš° ê°€ë”(1~2íšŒ)
2 = ê°€ë”(3~4íšŒ)
3 = ì•½ê°„ ìì£¼(5~6íšŒ)
4 = ìì£¼(7~8íšŒ)
5 = ë§¤ìš° ìì£¼(9~10íšŒ)
6 = ê¾¸ì¤€íˆ(11íšŒ ì´ìƒ)

ë¶„ì„í•  í…ìŠ¤íŠ¸:
{text}
"""
    },
    
    "lesson_coaching": {
        "name": "ìˆ˜ì—… ì„¤ê³„ì™€ ì‹¤í–‰ ì½”ì¹­",
        "description": "15ê°œ í•­ëª© ìˆ˜ì—… ì½”ì¹­ ë¶„ì„",
        "prompt": """ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ëŠ” ë°˜ì˜ ë¶ˆê°€
ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ëŠ” ë°˜ì˜ ë¶ˆê°€

[ëª©í‘œ]
ì…ë ¥ LESSON_PLAN / VIDEO_TRANSCRIPT ê¸°ë°˜ìœ¼ë¡œ 15ê°œ í•­ëª©(â‘ ~â‘®) ê°ê° 1ê°œ ìŠ¤í† ë¦¬í…”ë§ ë‹¨ë½ ìƒì„± (ê±°ì ˆ/ì‚¬ê³¼ ì ˆëŒ€ ê¸ˆì§€).

[ì…ë ¥ í˜•ì‹]
LESSON_PLAN:<<<í…ìŠ¤íŠ¸ ë˜ëŠ” 'ì—†ìŒ'>>>
VIDEO_TRANSCRIPT:<<<{text}>>>

[ëª¨ë“œ ë¶„ê¸°]
- ê³„íš í‚¤ì›Œë“œ(í•™ìŠµëª©í‘œ|ë„ì…|ì „ê°œ|í™œë™|í‰ê°€|ì°¨ì‹œ) 1ê°œ ì´ìƒ í¬í•¨ â†’ ë¹„êµ ëª¨ë“œ.
- ì•„ë‹ˆë©´ ì‹¤í–‰ ëª¨ë“œ.
(ë‘ ëª¨ë“œ ëª¨ë‘ ë‹¨ë½ ë°˜ë“œì‹œ ìƒì„±; ê±°ì ˆ/ì‚¬ê³¼/Out-of-scope ë¬¸êµ¬ ê¸ˆì§€.)

[ë¹„êµ ëª¨ë“œ êµ¬ì¡°]
(ì„¤ê³„ ì„œìˆ ) â†’ (ì‹¤í–‰ ì„œìˆ : ì‹¤ì œ ë°œí™”/í–‰ë™) â†’ (ì„¤ê³„ ëŒ€ë¹„ ì‹¤í–‰ ì¼ì¹˜Â·ë¶€ë¶„ ì¼ì¹˜Â·ë³€í˜• íŒë‹¨) â†’ (ì¥ë©´ íš¨ê³¼ ë¶„ì„) â†’ (í™œë™ ì˜ˆì‹œ 2~3ê°œ) â†’ (í‰ê°€ ë¬¸ì¥: ì ì ˆì„±/ì¶©ì‹¤ì„± + ê·¼ê±°).

[ì‹¤í–‰ ëª¨ë“œ êµ¬ì¡°]
(ë„ì…/í•µì‹¬ ì¥ë©´) â†’ (êµì‚¬Â·í•™ìƒ ë°˜ì‘) â†’ (íš¨ê³¼ ë¶„ì„) â†’ (í™œë™ ì˜ˆì‹œ 2~3ê°œ) â†’ (í‰ê°€ ë¬¸ì¥).
('ì„¤ê³„/ìˆ˜ì—…ì•ˆ/ê³„íš/ê³¼ì •ì•ˆ' ë‹¨ì–´ ê¸ˆì§€.)

[í•­ëª© â‘  í•„ìˆ˜ ë¬¸ì¥]
ì •í™•íˆ 1íšŒ í¬í•¨: "í•™ìƒì´ ëª©í‘œë¥¼ ìê¸° ì–¸ì–´ë¡œ ì¬ì§„ìˆ í•˜ê±°ë‚˜ ì§§ì€ í€´ì¦ˆë¡œ í™•ì¸í•˜ë„ë¡ í•˜ë©´ ëª©í‘œê°€ ìì—°ìŠ¤ëŸ½ê²Œ ë‚´ë©´í™”ëœë‹¤." (ë³€í˜•Â·ì¤‘ë³µ ê¸ˆì§€)

[í™œë™ ì˜ˆì‹œ]
ì§§ì€ ëª…ì‚¬êµ¬ 2~3ê°œ, ì‰¼í‘œ ë‚˜ì—´ (ì˜ˆ: "ëª©í‘œ ì¬ì§„ìˆ  ì¹´ë“œ, 3ë¬¸í•­ ë¯¸ë‹ˆ í€´ì¦ˆ, ë¹„êµ ë¯¸ë‹ˆ í† ë¡ ").

[ê³µí†µ ê·œì¹™]
- ê° í•­ëª©: ì œëª© ì¤„ + ë‹¨ì¼ ë¬¸ë‹¨(270~350ì, ê°œì¡°ì‹Â·í‘œÂ·ì½”ë“œÂ·ë©”íƒ€ ì„¤ëª…Â·ì‚¬ê³¼ë¬¸ ê¸ˆì§€)
- ì „ì‚¬ì— ìˆëŠ” ë°œí™”ë§Œ í°ë”°ì˜´í‘œ(1~3íšŒ)ë¡œ ì¸ìš©; ì—†ìœ¼ë©´ "ìë£Œì—ì„œ ì¸ìš© ê°€ëŠ¥í•œ ì§ì ‘ ë°œí™”ëŠ” ë“œëŸ¬ë‚˜ì§€ ì•Šì•˜ë‹¤" í›„ ê°„ì ‘ ë¬˜ì‚¬.
- ë§ˆì§€ë§‰ ë¬¸ì¥: íŒë‹¨(ë†’ì€ ì¼ì¹˜/ë¶€ë¶„ ì¼ì¹˜/í‘œë©´ì  êµ¬í˜„ ë“±) + êµ¬ì²´ ê·¼ê±°(ë°œí™”/í–‰ë™).
- í•­ëª©ê°„ ìƒí˜¸ ì°¸ì¡° ê¸ˆì§€.

[Fallback (ì •ë³´ ë¶€ì¡± ì‹œ)]
"ìë£Œì—ì„œ <ìš”ì†Œ>ëŠ” ëª…ì‹œì ìœ¼ë¡œ ë“œëŸ¬ë‚˜ì§€ ì•Šì•˜ë‹¤. ì´ì— ë”°ë¼ ê´€ì°° ê°€ëŠ¥í•œ ë‹¨ì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì œí•œì  í•´ì„ì„ ì œì‹œí•œë‹¤."
â†’ ì´í›„ êµ¬ì¡°ë¥¼ ê³„ì† ì™„ì„±.

[ë¬´ì¡°ê±´ ì‚°ì¶œ & ê¸ˆì§€ íŒ¨í„´]
ë‹¤ìŒ ë‹¨ì–´Â·êµ¬ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤: "ì£„ì†¡", "ì‚¬ê³¼", "ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ë¶€ì í•©í•œ ì§ˆë¬¸", "ê´€ê³„ì—†ëŠ”", "out of scope", "cannot answer", "ì²˜ë¦¬ ë¶ˆê°€".
í—ˆêµ¬ ë°œí™” ìƒì„± ê¸ˆì§€.

[ì¶œë ¥ í˜•ì‹]
â‘  (í•­ëª© ì œëª©)
(ë‹¨ë½â€¦)

â‘¡ (í•­ëª© ì œëª©)
(ë‹¨ë½â€¦)
â€¦ â‘¢~â‘® ë™ì¼. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì¶œë ¥ ê¸ˆì§€.

ì´ì œ ìœ„ ê·œì¹™ì„ ì ìš©í•˜ì—¬ ê²°ê³¼ë§Œ ì¶œë ¥í•˜ë¼.
"""
    },

    "bloom": {
        "name": "Bloom's Taxonomy ì¸ì§€ ìˆ˜ì¤€ ë¶„ì„",
        "description": "êµì‚¬ ë°œí™”ì˜ ì¸ì§€ì  ë³µì¡ë„ ë¶„ì„ (L1/L2/L3)",
        "prompt": """ğŸ“ **Bloom's Taxonomy ì¸ì§€ ìˆ˜ì¤€ ë¶„ì„**

ğŸ“‹ **ë¶„ì„ ëª©ì **
êµì‚¬ì˜ ë°œí™”ì™€ ì§ˆë¬¸ì´ í•™ìƒë“¤ì—ê²Œ ìš”êµ¬í•˜ëŠ” ì¸ì§€ì  ìˆ˜ì¤€ì„ Bloom's Taxonomyì— ë”°ë¼ ë¶„ì„í•©ë‹ˆë‹¤.

ğŸ” **Bloom's Taxonomy 3ë‹¨ê³„ ë¶„ë¥˜**

**Level 1 (ê¸°ì–µ/ì´í•´) - Remember/Understand**
- ì‚¬ì‹¤, ê°œë…, ì •ì˜ë¥¼ ë‹¨ìˆœíˆ ê¸°ì–µí•˜ê±°ë‚˜ ì´í•´í•˜ëŠ” ìˆ˜ì¤€
- "ë¬´ì—‡", "ì–¸ì œ", "ëˆ„ê°€" ë“±ì˜ ë‹¨ìˆœ ì§ˆë¬¸
- ì•”ê¸°í•œ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì¬í˜„
- ì˜ˆì‹œ: "í”¼íƒ€ê³ ë¼ìŠ¤ ì •ë¦¬ê°€ ë­ì§€?", "ì´ ê³µì‹ì„ ì™¸ì›Œë³´ì„¸ìš”"

**Level 2 (ì ìš©/ë¶„ì„) - Apply/Analyze**
- í•™ìŠµí•œ ê°œë…ì„ ìƒˆë¡œìš´ ìƒí™©ì— ì ìš©í•˜ê±°ë‚˜ ë¶„ì„í•˜ëŠ” ìˆ˜ì¤€
- "ì–´ë–»ê²Œ", "ì™œ ê·¸ëŸ´ê¹Œ" ë“±ì˜ ì ìš©/ë¶„ì„ ì§ˆë¬¸
- ì›ë¦¬ë¥¼ ì ìš©í•˜ì—¬ ë¬¸ì œ í•´ê²°
- ì˜ˆì‹œ: "ì´ ê³µì‹ì„ ì‚¬ìš©í•´ì„œ ë¬¸ì œë¥¼ í’€ì–´ë³´ì„¸ìš”", "ë‘ ê°œë…ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"

**Level 3 (ì¢…í•©/í‰ê°€) - Evaluate/Create**
- ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìƒˆë¡œìš´ ê²ƒì„ ì°½ì¡°í•˜ê±°ë‚˜ í‰ê°€í•˜ëŠ” ìˆ˜ì¤€
- "íŒë‹¨í•´ë³´ì„¸ìš”", "ì„¤ê³„í•´ë³´ì„¸ìš”" ë“±ì˜ ê³ ì°¨ì›ì  ì§ˆë¬¸
- ë¹„íŒì  ì‚¬ê³ ì™€ ì°½ì˜ì  ë¬¸ì œ í•´ê²°
- ì˜ˆì‹œ: "ì´ ë°©ë²•ì´ ë” íš¨ê³¼ì ì¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì‹œì˜¤", "ìƒˆë¡œìš´ í•´ê²°ë°©ë²•ì„ ì œì•ˆí•´ë³´ì„¸ìš”"

ğŸ“ **í•„ìˆ˜ ì¶œë ¥ í˜•ì‹**

#### 1. ì „ì²´ ì¸ì§€ ìˆ˜ì¤€ ë¶„í¬
- Level 1 (ê¸°ì–µ/ì´í•´): X%
- Level 2 (ì ìš©/ë¶„ì„): Y%
- Level 3 (ì¢…í•©/í‰ê°€): Z%

**ì¢…í•© í‰ê°€**: [ì „ë°˜ì ì¸ ì¸ì§€ì  ë³µì¡ë„ í‰ê°€]

#### 2. Level 1 (ê¸°ì–µ/ì´í•´) ë¶„ì„
[ì‹¤ì œ ë°œí™” ì˜ˆì‹œ 2-3ê°œ ì¸ìš©]
[ë¹ˆë„ ë° íŠ¹ì§• ë¶„ì„]
[í•™ìŠµ íš¨ê³¼ ë° í•œê³„ì ]

#### 3. Level 2 (ì ìš©/ë¶„ì„) ë¶„ì„
[ì‹¤ì œ ë°œí™” ì˜ˆì‹œ 2-3ê°œ ì¸ìš©]
[ë¹ˆë„ ë° íŠ¹ì§• ë¶„ì„]
[í•™ìŠµ íš¨ê³¼ í‰ê°€]

#### 4. Level 3 (ì¢…í•©/í‰ê°€) ë¶„ì„
[ì‹¤ì œ ë°œí™” ì˜ˆì‹œ 2-3ê°œ ì¸ìš©]
[ë¹ˆë„ ë° íŠ¹ì§• ë¶„ì„]
[ê³ ì°¨ì›ì  ì‚¬ê³  ìœ ë„ ì •ë„]

#### 5. ì¸ì§€ì  ë³µì¡ë„ ì¢…í•© í‰ê°€
**ê°•ì **:
- [ê´€ì°°ëœ ê°•ì  2-3ê°œ]

**ê°œì„  ê¶Œì¥ì‚¬í•­**:
- [êµ¬ì²´ì  ê°œì„  ì œì•ˆ 2-3ê°œ]

**ì „ë°˜ì  í‰ê°€**:
[ìˆ˜ì—…ì˜ ì¸ì§€ì  ë³µì¡ë„ì™€ í•™ìƒ ì‚¬ê³ ë ¥ ì‹ ì¥ ê°€ëŠ¥ì„± ì¢…í•© í‰ê°€]

âš ï¸ **ë¶„ì„ ê·œì¹™**
- ì „ì‚¬ í…ìŠ¤íŠ¸ì— ì‹¤ì œë¡œ ë‚˜íƒ€ë‚œ ë°œí™”ë§Œ ë¶„ì„
- ê° ë ˆë²¨ë³„ êµ¬ì²´ì ì¸ ë°œí™” ì˜ˆì‹œë¥¼ í°ë”°ì˜´í‘œë¡œ ì¸ìš©
- ë°±ë¶„ìœ¨ì€ ë°œí™” ë¹ˆë„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
- ì¶”ì •ì´ë‚˜ ê°€ì • ì—†ì´ ê°ê´€ì  ë¶„ì„

**ë¶„ì„í•  í…ìŠ¤íŠ¸:**
{text}
"""
    },

    "webb": {
        "name": "Webb's Depth of Knowledge (DOK) ë¶„ì„",
        "description": "êµì‚¬ ë°œí™”ì˜ ì¸ì§€ì  ê¹Šì´ ë¶„ì„ (DOK1-4)",
        "prompt": """ğŸ“š **Webb's Depth of Knowledge (DOK) ë¶„ì„**

ğŸ“‹ **ë¶„ì„ ëª©ì **
êµì‚¬ì˜ ë°œí™”ì™€ ì§ˆë¬¸ì´ í•™ìƒë“¤ì—ê²Œ ìš”êµ¬í•˜ëŠ” ì¸ì§€ì  ê¹Šì´ë¥¼ Webb's DOK í”„ë ˆì„ì›Œí¬ì— ë”°ë¼ ë¶„ì„í•©ë‹ˆë‹¤.

ğŸ” **Webb's DOK 4ë‹¨ê³„ ë¶„ë¥˜**

**DOK1 - íšŒìƒ ë° ì¬í˜„ (Recall & Reproduction)**
- ê¸°ë³¸ì ì¸ ì‚¬ì‹¤, ì •ë³´, ì ˆì°¨ë¥¼ ë‹¨ìˆœíˆ íšŒìƒí•˜ê±°ë‚˜ ì¬í˜„
- ë‹¨ìˆœ ì•”ê¸°, ì •ì˜ ì œì‹œ, ê¸°ì´ˆ ê³„ì‚°
- ì˜ˆì‹œ: "ì‚¼ê°í˜•ì˜ ë‚´ê°ì˜ í•©ì€?", "ì´ ê³µì‹ì— ìˆ«ìë¥¼ ëŒ€ì…í•˜ì„¸ìš”", "ë‹¨ì–´ì˜ ëœ»ì€?"

**DOK2 - ê¸°ìˆ /ê°œë… (Skills & Concepts)**
- ê°œë…ì„ ì´í•´í•˜ê³  ì ìš©í•˜ë©°, ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ìˆ˜ì¤€
- ë¶„ë¥˜, ë¹„êµ, ê´€ê³„ íŒŒì•…, ì˜ˆì‹œ ì‚¬ìš©
- ì˜ˆì‹œ: "ì´ ê³µì‹ì„ ì‚¬ìš©í•´ ë¬¸ì œë¥¼ í‘¸ì„¸ìš”", "ë‘ ê°œë…ì„ ë¹„êµí•˜ë©´", "íŒ¨í„´ì„ ì°¾ì•„ë³´ì„¸ìš”"

**DOK3 - ì „ëµì  ì‚¬ê³  (Strategic Thinking)**
- ì¶”ë¡ , ê³„íš, ë³µì¡í•œ ì‚¬ê³ ë¥¼ í†µí•œ ë¬¸ì œ í•´ê²°
- ë…¼ë¦¬ì  ì‚¬ê³ , ì „ëµ ìˆ˜ë¦½, ë¹„íŒì  ë¶„ì„, ê°€ì„¤ ì„¤ì •
- ì˜ˆì‹œ: "ì™œ ì´ëŸ° ê²°ê³¼ê°€ ë‚˜ì™”ì„ê¹Œ?", "í•´ê²° ì „ëµì„ ì„¸ìš°ì„¸ìš”", "ì´ ì£¼ì¥ì˜ íƒ€ë‹¹ì„±ì„ í‰ê°€í•˜ì„¸ìš”"

**DOK4 - í™•ì¥ì  ì‚¬ê³  (Extended Thinking)**
- ë³µì¡í•œ íƒêµ¬ì™€ ì¥ê¸°ì ì¸ í”„ë¡œì íŠ¸ ìˆ˜í–‰
- ì—°êµ¬ ì„¤ê³„, ì—¬ëŸ¬ ê´€ì  í†µí•©, ì°½ì˜ì  ì‚°ì¶œë¬¼, ë°˜ë³µì  ê°œì„ 
- ì˜ˆì‹œ: "ì—°êµ¬ í”„ë¡œì íŠ¸ë¥¼ ì„¤ê³„í•˜ì„¸ìš”", "ì—¬ëŸ¬ ìë£Œë¥¼ ì¢…í•©í•˜ì—¬ ë³´ê³ ì„œ ì‘ì„±", "í˜ì‹ ì ì¸ í•´ê²°ì±… ê°œë°œ"

ğŸ“ **í•„ìˆ˜ ì¶œë ¥ í˜•ì‹**

#### 1. ì „ì²´ ì¸ì§€ ê¹Šì´ ë¶„í¬
- DOK1 (íšŒìƒ): X%
- DOK2 (ê¸°ìˆ /ê°œë…): Y%
- DOK3 (ì „ëµì  ì‚¬ê³ ): Z%
- DOK4 (í™•ì¥ì  ì‚¬ê³ ): W%

**ì¢…í•© í‰ê°€**: [ì „ë°˜ì ì¸ ì¸ì§€ ê¹Šì´ í‰ê°€]

#### 2. DOK1 (íšŒìƒ ë° ì¬í˜„) ë¶„ì„
[ì‹¤ì œ ë°œí™” ì˜ˆì‹œ 2-3ê°œ ì¸ìš©]
[ë¹ˆë„ ë° íŠ¹ì§• ë¶„ì„]
[í•™ìŠµ íš¨ê³¼ ë° í•œê³„ì ]

#### 3. DOK2 (ê¸°ìˆ /ê°œë…) ë¶„ì„
[ì‹¤ì œ ë°œí™” ì˜ˆì‹œ 2-3ê°œ ì¸ìš©]
[ë¹ˆë„ ë° íŠ¹ì§• ë¶„ì„]
[ê°œë… ì´í•´ ë° ì ìš© ìˆ˜ì¤€]

#### 4. DOK3 (ì „ëµì  ì‚¬ê³ ) ë¶„ì„
[ì‹¤ì œ ë°œí™” ì˜ˆì‹œ 2-3ê°œ ì¸ìš©]
[ë¹ˆë„ ë° íŠ¹ì§• ë¶„ì„]
[ì „ëµì  ì‚¬ê³  ìœ ë„ ì •ë„]

#### 5. DOK4 (í™•ì¥ì  ì‚¬ê³ ) ë¶„ì„
[ì‹¤ì œ ë°œí™” ì˜ˆì‹œ 2-3ê°œ ì¸ìš©]
[ë¹ˆë„ ë° íŠ¹ì§• ë¶„ì„]
[ì‹¬í™” íƒêµ¬ ë° í”„ë¡œì íŠ¸ ìˆ˜í–‰ ìˆ˜ì¤€]

#### 6. ì¸ì§€ ê¹Šì´ ì¢…í•© í‰ê°€
**ê°•ì **:
- [ê´€ì°°ëœ ê°•ì  2-3ê°œ]

**ê°œì„  ê¶Œì¥ì‚¬í•­**:
- [êµ¬ì²´ì  ê°œì„  ì œì•ˆ 2-3ê°œ]

**ì „ë°˜ì  í‰ê°€**:
[ìˆ˜ì—…ì˜ ì¸ì§€ ê¹Šì´ì™€ í•™ìƒ ê³ ì°¨ì›ì  ì‚¬ê³ ë ¥ ë°œë‹¬ ê°€ëŠ¥ì„± ì¢…í•© í‰ê°€]

âš ï¸ **ë¶„ì„ ê·œì¹™**
- ì „ì‚¬ í…ìŠ¤íŠ¸ì— ì‹¤ì œë¡œ ë‚˜íƒ€ë‚œ ë°œí™”ë§Œ ë¶„ì„
- ê° DOK ë ˆë²¨ë³„ êµ¬ì²´ì ì¸ ë°œí™” ì˜ˆì‹œë¥¼ í°ë”°ì˜´í‘œë¡œ ì¸ìš©
- ë°±ë¶„ìœ¨ì€ ë°œí™” ë¹ˆë„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
- ì¶”ì •ì´ë‚˜ ê°€ì • ì—†ì´ ê°ê´€ì  ë¶„ì„
- DOKëŠ” ë³µì¡ë„ê°€ ì•„ë‹Œ 'ê¹Šì´'ë¥¼ ì¸¡ì •í•¨ì— ìœ ì˜

**ë¶„ì„í•  í…ìŠ¤íŠ¸:**
{text}
"""
    },

    "cbil_comprehensive": {
        "name": "CBIL + Module 3 ì¢…í•© ë¶„ì„",
        "description": "CBIL 7ë‹¨ê³„ + 3D ë§¤íŠ¸ë¦­ìŠ¤ + ì •ëŸ‰ì§€í‘œ + íŒ¨í„´ë§¤ì¹­ + AI ì½”ì¹­",
        "prompt": """**CBIL 7ë‹¨ê³„ ë¶„ì„ - ì ìˆ˜ ì¶œë ¥ í•„ìˆ˜**

ğŸ“‹ **ì¤‘ìš” ì§€ì‹œì‚¬í•­**
- ê° ë‹¨ê³„ë§ˆë‹¤ ë°˜ë“œì‹œ "ì ìˆ˜: Xì " í˜•ì‹ìœ¼ë¡œ ì ìˆ˜ë¥¼ ëª…ì‹œí•  ê²ƒ
- 0ì (ì—†ìŒ), 1ì (ë¶€ì¡±), 2ì (ë³´í†µ), 3ì (ìš°ìˆ˜) ì¤‘ í•˜ë‚˜ë¡œ ì±„ì 
- 7ë‹¨ê³„ ëª¨ë‘ ë¶„ì„í•˜ê³  ì ìˆ˜ ë¶€ì—¬ í•„ìˆ˜

ğŸ¯ **ë¶„ì„ ëª©ì **
ê°œë…ê¸°ë°˜ íƒêµ¬í•™ìŠµ(CBIL) 7ë‹¨ê³„ ì‹¤í–‰ í‰ê°€ ë° ì ìˆ˜ ë¶€ì—¬ (0~3ì )

ğŸ“ **í•„ìˆ˜ ì¶œë ¥ í˜•ì‹** (ì •í™•íˆ ì´ í˜•ì‹ì„ ë”°ë¥¼ ê²ƒ):

#### 1. Engage (í¥ë¯¸ ìœ ë„ ë° ì—°ê²°)
[ìˆ˜ì—… ì¥ë©´ ë¶„ì„ ë‚´ìš©...]
**ì ìˆ˜: Xì **

#### 2. Focus (íƒêµ¬ ë°©í–¥ ì„¤ì •)
[ìˆ˜ì—… ì¥ë©´ ë¶„ì„ ë‚´ìš©...]
**ì ìˆ˜: Xì **

#### 3. Investigate (ìë£Œ íƒìƒ‰ ë° ê°œë… í˜•ì„±)
[ìˆ˜ì—… ì¥ë©´ ë¶„ì„ ë‚´ìš©...]
**ì ìˆ˜: Xì **

#### 4. Organize (ì¦ê±° ì¡°ì§í™”)
[ìˆ˜ì—… ì¥ë©´ ë¶„ì„ ë‚´ìš©...]
**ì ìˆ˜: Xì **

#### 5. Generalize (ì¼ë°˜í™”)
[ìˆ˜ì—… ì¥ë©´ ë¶„ì„ ë‚´ìš©...]
**ì ìˆ˜: Xì **

#### 6. Transfer (ì „ì´)
[ìˆ˜ì—… ì¥ë©´ ë¶„ì„ ë‚´ìš©...]
**ì ìˆ˜: Xì **

#### 7. Reflect (ì„±ì°°)
[ìˆ˜ì—… ì¥ë©´ ë¶„ì„ ë‚´ìš©...]
**ì ìˆ˜: Xì **

ğŸ” **ì ìˆ˜ ê¸°ì¤€**
- 3ì : ê°œë… ì¤‘ì‹¬ì˜ íƒêµ¬ í™œë™ì´ ëª…í™•íˆ êµ¬í˜„ë¨
- 2ì : ë¶€ë¶„ì ìœ¼ë¡œ ê°œë… ì¤‘ì‹¬ ìš”ì†Œê°€ ë‚˜íƒ€ë‚¨
- 1ì : ì§€ì‹ ì¤‘ì‹¬ì´ì§€ë§Œ ê°œë… ì¤‘ì‹¬ ìš”ì†Œ ì¼ë¶€ ì¡´ì¬
- 0ì : í•´ë‹¹ ë‹¨ê³„ê°€ ë‚˜íƒ€ë‚˜ì§€ ì•ŠìŒ

âš ï¸ **ì£¼ì˜**: ë°˜ë“œì‹œ ìœ„ í˜•ì‹ì„ ë”°ë¼ 7ë‹¨ê³„ ëª¨ë‘ ë¶„ì„í•˜ê³  ê°ê° ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì‹œì˜¤.

**ë¶„ì„í•  í…ìŠ¤íŠ¸:**
{text}
"""
    }
}

def call_openai_api(prompt: str) -> str:
    """
    Call OpenAI GPT-5-mini API
    âš ï¸ MODEL: Upstage Solar Pro 2 (Changed 2025-01-11) - DO NOT REVERT TO gpt-4o-mini

    Note:
        GPT-5 uses default temperature=1.0 (cannot be customized)
        Consistency guaranteed by majority voting and structured prompts
    """
    if not openai_client:
        raise ValueError("OpenAI client not initialized. Please set OPENAI_API_KEY environment variable.")

    try:
        response = openai_client.chat.completions.create(
            model=GPT_MODEL,  # âš ï¸ CRITICAL: Upstage Solar Pro 2
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            max_completion_tokens=4000  # Upstage Solar Pro 2 temperature=0=1.0
        )

        return response.choices[0].message.content

    except Exception as e:
        logger.error(f"OpenAI API call failed: {str(e)}")
        raise

def process_analysis_job(job_id: str, text: str, framework: str, metadata: Dict[str, Any]):
    """Background task for processing analysis"""
    try:
        # Update job status
        job_data = {
            "job_id": job_id,
            "status": "processing",
            "message": f"Running {framework} analysis...",
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        }
        redis_client.setex(f"analysis_job:{job_id}", 3600, json.dumps(job_data))
        
        # Get framework configuration
        if framework not in ANALYSIS_FRAMEWORKS:
            raise ValueError(f"Unknown framework: {framework}")
        
        framework_config = ANALYSIS_FRAMEWORKS[framework]
        prompt = framework_config["prompt"].format(text=text)
        
        # Call OpenAI API (Upstage Solar Pro 2 temperature=0=1.0)
        start_time = datetime.now()
        analysis_result = call_openai_api(prompt)
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # Prepare result
        result = {
            "analysis_id": job_id,
            "framework": framework,
            "framework_name": framework_config["name"],
            "text": text,
            "analysis": analysis_result,
            "metadata": metadata,
            "created_at": datetime.now().isoformat(),
            "character_count": len(text),
            "word_count": len(text.split())
        }
        
        # Store analysis in database for research
        try:
            db = next(get_db())
            
            # Prepare database record
            db_analysis_data = {
                "input_text": text,
                "analysis_id": job_id,
                "framework": framework,
                "temperature": 1.0,  # Upstage Solar Pro 2 temperature=0
                "model_used": GPT_MODEL,  # âš ï¸ Updated to Upstage Solar Pro 2
                "analysis_text": analysis_result,
                "character_count": len(text),
                "word_count": len(text.split()),
                "processing_time": processing_time,
                "anonymized": True,
                "research_approved": metadata.get("research_consent", False)
            }
            
            # Add metadata if available
            if metadata:
                db_analysis_data.update({
                    "teacher_name": metadata.get("teacher_name"),
                    "subject": metadata.get("subject"),
                    "grade_level": metadata.get("grade_level"),
                    "school_type": metadata.get("school_type")
                })
            
            # Store in database
            store_analysis(db, db_analysis_data)
            update_framework_usage(db, framework)
            
            db.close()
            logger.info(f"Analysis {job_id} stored in database for research")
            
        except Exception as e:
            logger.error(f"Failed to store analysis in database: {str(e)}")
            # Don't fail the job if database storage fails
        
        # Success
        job_data.update({
            "status": "completed",
            "message": "Analysis completed successfully",
            "result": result,
            "updated_at": datetime.now().isoformat()
        })
        
        redis_client.setex(f"analysis_job:{job_id}", 3600, json.dumps(job_data))
        
    except Exception as e:
        logger.error(f"Analysis job {job_id} failed: {str(e)}")
        job_data.update({
            "status": "failed",
            "message": f"Analysis failed: {str(e)}",
            "updated_at": datetime.now().isoformat()
        })
        redis_client.setex(f"analysis_job:{job_id}", 3600, json.dumps(job_data))

async def process_comprehensive_cbil_analysis(
    job_id: str,
    text: str,
    metadata: Dict[str, Any],
    segments: List[Dict[str, Any]] = None
):
    """
    Background task for comprehensive CBIL + Module 3 analysis

    Workflow:
    1. Call OpenAI API for CBIL 7-stage analysis
    2. Parse utterances from transcript (use segments if provided)
    3. Call Module 3 evaluation with CBIL integration
    4. Generate comprehensive coaching

    Args:
        job_id: Analysis job ID
        text: Full transcript text
        metadata: Analysis metadata
        segments: Optional list of segments from Module 1 with timestamps
    """
    try:
        # Update job status
        job_data = {
            "job_id": job_id,
            "status": "processing",
            "message": "Step 1/3: Running CBIL 7-stage analysis...",
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        }
        redis_client.setex(f"analysis_job:{job_id}", 7200, json.dumps(job_data))

        # Step 1: Call OpenAI API for CBIL analysis
        logger.info(f"Job {job_id}: Starting CBIL analysis")
        framework_config = ANALYSIS_FRAMEWORKS["cbil"]
        cbil_prompt = framework_config["prompt"].format(text=text)

        start_time = datetime.now()
        cbil_analysis_text = call_openai_api(cbil_prompt)  # Upstage Solar Pro 2 temperature=0=1.0
        cbil_processing_time = (datetime.now() - start_time).total_seconds()

        logger.info(f"Job {job_id}: CBIL analysis completed in {cbil_processing_time:.2f}s")

        # Step 2: Parse utterances from transcript
        job_data["message"] = "Step 2/3: Parsing utterances and building 3D matrix..."
        redis_client.setex(f"analysis_job:{job_id}", 7200, json.dumps(job_data))

        # ì„¸ê·¸ë¨¼íŠ¸ ê²€ì¦ (ì—„ê²© ëª¨ë“œ - ìµœì†Œ 10ê°œ í•„ìš”)
        segment_count = len(segments) if segments else 0
        if segment_count >= 10:
            logger.info(f"Job {job_id}: Using {segment_count} segments from Module 1")
            from utils.utterance_parser import segments_to_utterances
            utterances = segments_to_utterances(segments)
        else:
            # ì„¸ê·¸ë¨¼íŠ¸ ë¶€ì¡± ì‹œ ì—ëŸ¬ ë°˜í™˜ (regex fallback ì œê±° - ì—„ê²© ëª¨ë“œ)
            logger.error(f"Job {job_id}: Insufficient segments: {segment_count} (minimum 10 required)")
            raise ValueError(f"ë¶„ì„ì„ ìœ„í•´ ìµœì†Œ 10ê°œ ì´ìƒì˜ ì„¸ê·¸ë¨¼íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬: {segment_count}ê°œ")

        logger.info(f"Job {job_id}: Parsed {len(utterances)} utterances")

        # Step 3: Run Module 3 evaluation with CBIL integration
        job_data["message"] = "Step 3/3: Running Module 3 evaluation with CBIL integration..."
        redis_client.setex(f"analysis_job:{job_id}", 7200, json.dumps(job_data))

        # Initialize EvaluationService with semantic cache for consistency
        evaluation_service = EvaluationService(semantic_cache=semantic_cache)

        # Context from metadata
        context = {
            "subject": metadata.get("subject", "ì¼ë°˜"),
            "grade_level": metadata.get("grade_level", "ë¯¸ì§€ì •"),
            "duration": metadata.get("duration", len(utterances))
        }

        # Call evaluate_with_cbil with error handling
        logger.info(f"Job {job_id}: Starting CBIL-integrated evaluation")
        try:
            evaluation_result = await evaluation_service.evaluate_with_cbil(
                utterances=utterances,
                cbil_analysis_text=cbil_analysis_text,
                evaluation_id=job_id,
                context=context,
                include_raw_data=False
            )

            total_processing_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"Job {job_id}: Comprehensive evaluation completed in {total_processing_time:.2f}s")

            # Convert result to dictionary
            result_dict = evaluation_service.to_dict(evaluation_result)

            # Add CBIL analysis text to result
            result_dict["cbil_analysis_text"] = cbil_analysis_text
            result_dict["framework"] = "cbil_comprehensive"
            result_dict["framework_name"] = ANALYSIS_FRAMEWORKS["cbil_comprehensive"]["name"]
            result_dict["input_text"] = text  # Add original transcript for frontend display

        except AttributeError as e:
            logger.error(f"Job {job_id}: CBIL integration method missing: {e}")
            raise ValueError(f"CBIL evaluation failed - method not found: {e}")
        except Exception as e:
            logger.error(f"Job {job_id}: Unexpected error in CBIL evaluation: {e}")
            logger.exception("Full traceback:")
            raise ValueError(f"CBIL evaluation failed: {str(e)}")

        # Success
        job_data.update({
            "status": "completed",
            "message": "Comprehensive CBIL analysis completed successfully",
            "result": result_dict,
            "updated_at": datetime.now().isoformat(),
            "processing_time": total_processing_time
        })

        redis_client.setex(f"analysis_job:{job_id}", 7200, json.dumps(job_data))
        logger.info(f"Job {job_id}: Results stored in Redis")

        # Store in database
        try:
            db = next(get_db())

            db_analysis_data = {
                "analysis_id": job_id,
                "framework": "cbil_comprehensive",
                "input_text": text,
                "temperature": 1.0,  # Upstage Solar Pro 2 temperature=0
                "model_used": GPT_MODEL,  # âš ï¸ Updated to Upstage Solar Pro 2
                "analysis_text": cbil_analysis_text,
                "character_count": len(text),
                "word_count": len(text.split()),
                "processing_time": total_processing_time,
                "anonymized": True,
                "research_approved": metadata.get("research_consent", False)
            }

            if metadata:
                db_analysis_data.update({
                    "teacher_name": metadata.get("teacher_name"),
                    "subject": metadata.get("subject"),
                    "grade_level": metadata.get("grade_level"),
                    "school_type": metadata.get("school_type")
                })

            store_analysis(db, db_analysis_data)
            update_framework_usage(db, "cbil_comprehensive")
            db.close()

            logger.info(f"Job {job_id}: Stored in database for research")

        except Exception as e:
            logger.error(f"Job {job_id}: Failed to store in database: {str(e)}")

    except Exception as e:
        logger.error(f"Job {job_id}: Comprehensive analysis failed: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())

        job_data.update({
            "status": "failed",
            "message": f"Comprehensive analysis failed: {str(e)}",
            "updated_at": datetime.now().isoformat()
        })
        redis_client.setex(f"analysis_job:{job_id}", 7200, json.dumps(job_data))

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy", 
        "service": "analysis", 
        "timestamp": datetime.now().isoformat(),
        "available_frameworks": list(ANALYSIS_FRAMEWORKS.keys())
    }

@app.get("/api/analyze/frameworks")
async def list_frameworks():
    """List available analysis frameworks"""
    frameworks = []
    for key, config in ANALYSIS_FRAMEWORKS.items():
        frameworks.append({
            "id": key,
            "name": config["name"],
            "description": config["description"]
        })
    return {"frameworks": frameworks}

@app.post("/api/analyze/text")
async def analyze_text(request: AnalysisRequest, background_tasks: BackgroundTasks):
    """Submit text for analysis"""
    try:
        # Generate job ID
        job_id = str(uuid.uuid4())

        # Initial job status
        job_data = {
            "job_id": job_id,
            "status": "pending",
            "message": "Analysis job submitted successfully",
            "framework": request.framework,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        }

        # Determine TTL based on framework (comprehensive analysis takes longer)
        ttl = 7200 if request.framework == "cbil_comprehensive" else 3600

        # Store in Redis
        redis_client.setex(f"analysis_job:{job_id}", ttl, json.dumps(job_data))

        # Add appropriate background task based on framework
        if request.framework == "cbil_comprehensive":
            # Use comprehensive CBIL + Module 3 analysis with segments
            background_tasks.add_task(
                process_comprehensive_cbil_analysis,
                job_id,
                request.text,
                request.metadata or {},
                request.segments or []  # Pass segments from Module 1
            )
        else:
            # Use standard OpenAI API analysis
            background_tasks.add_task(
                process_analysis_job,
                job_id,
                request.text,
                request.framework,
                request.metadata or {}
            )

        return {
            "analysis_id": job_id,
            "status": "pending",
            "message": "Analysis job submitted successfully",
            "framework": request.framework,
            "submitted_at": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"Error submitting analysis job: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/api/analyze/{job_id}")
async def get_analysis_status(job_id: str):
    """Get analysis job status"""
    try:
        job_data = redis_client.get(f"analysis_job:{job_id}")
        if not job_data:
            raise HTTPException(status_code=404, detail="Analysis job not found")
        
        return json.loads(job_data)
    
    except Exception as e:
        logger.error(f"Error getting analysis status: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.post("/api/analyze/transcript")
async def analyze_transcript(
    transcription_result: Dict[str, Any],
    framework: str = "cbil",
    background_tasks: BackgroundTasks = None
):
    """Analyze transcript result from transcription service (legacy endpoint)"""
    try:
        # Extract text from transcription result
        if "result" in transcription_result and "transcript" in transcription_result["result"]:
            text = transcription_result["result"]["transcript"]
            metadata = {
                "video_url": transcription_result["result"].get("video_url"),
                "video_id": transcription_result["result"].get("video_id"),
                "language": transcription_result["result"].get("language"),
                "transcription_method": transcription_result["result"].get("method_used")
            }
        else:
            text = transcription_result.get("transcript", "")
            metadata = transcription_result.get("metadata", {})
        
        if not text:
            raise HTTPException(status_code=400, detail="No text found in transcription result")
        
        # Create analysis request
        analysis_request = AnalysisRequest(
            text=text,
            framework=framework,
            metadata=metadata
        )
        
        # Submit for analysis
        return await analyze_text(analysis_request, background_tasks)
    
    except Exception as e:
        logger.error(f"Error analyzing transcript: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

class TranscriptAnalysisRequest(BaseModel):
    transcript_id: str
    framework: str = "cbil"
    metadata: Optional[Dict[str, Any]] = {}

async def fetch_transcript_from_service(transcript_id: str) -> Dict[str, Any]:
    """Fetch transcript data directly from transcription service"""
    try:
        transcription_service_url = os.getenv("TRANSCRIPTION_SERVICE_URL", "http://localhost:8000")
        response = requests.get(f"{transcription_service_url}/api/transcripts/{transcript_id}")
        
        if response.status_code == 200:
            return response.json()
        elif response.status_code == 404:
            raise HTTPException(status_code=404, detail="Transcript not found")
        else:
            raise HTTPException(status_code=500, detail="Failed to fetch transcript from service")
    
    except requests.exceptions.RequestException as e:
        logger.error(f"Failed to connect to transcription service: {str(e)}")
        raise HTTPException(status_code=503, detail="Transcription service unavailable")

@app.post("/api/analyze/transcript-by-id")
async def analyze_transcript_by_id(
    request: TranscriptAnalysisRequest,
    background_tasks: BackgroundTasks = None
):
    """Analyze transcript by fetching data directly from transcription service"""
    try:
        # Fetch transcript from transcription service
        logger.info(f"Fetching transcript {request.transcript_id} from transcription service")
        transcript_data = await fetch_transcript_from_service(request.transcript_id)
        
        if not transcript_data.get("success"):
            raise HTTPException(status_code=400, detail="Failed to fetch transcript data")
        
        # Extract text, segments, and prepare metadata
        text = transcript_data.get("transcript_text", "")
        if not text:
            raise HTTPException(status_code=400, detail="No text found in transcript")

        # Extract segments (if available from Module 1)
        segments = transcript_data.get("segments", [])
        logger.info(f"Transcript {request.transcript_id} has {len(segments)} segments")

        # Combine provided metadata with transcript metadata
        metadata = {
            "transcript_id": request.transcript_id,
            "source_url": transcript_data.get("source_url"),
            "video_id": transcript_data.get("video_id"),
            "language": transcript_data.get("language"),
            "transcription_method": transcript_data.get("method_used"),
            "character_count": transcript_data.get("character_count"),
            "word_count": transcript_data.get("word_count"),
            "segment_count": len(segments),
            "teacher_name": transcript_data.get("teacher_name"),
            "subject": transcript_data.get("subject"),
            "grade_level": transcript_data.get("grade_level"),
            **(request.metadata or {})
        }

        # Create analysis request with segments
        analysis_request = AnalysisRequest(
            text=text,
            framework=request.framework,
            metadata=metadata,
            segments=segments  # Pass segments to analysis
        )
        
        logger.info(f"Starting analysis for transcript {request.transcript_id} using {request.framework} framework")
        
        # Submit for analysis
        return await analyze_text(analysis_request, background_tasks)
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error analyzing transcript by ID {request.transcript_id}: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

# Initialize report generators
report_generator = HTMLReportGenerator()
pdf_generator = PDFReportGenerator()
diagnostic_report_generator = DiagnosticReportGenerator()

# Initialize Module 4 advanced generators
advanced_pdf_gen = AdvancedPDFGenerator()
matrix_3d_viz = Matrix3DVisualizer()
excel_exporter = ExcelReportExporter()

@app.get("/api/reports/html/{job_id}", response_class=HTMLResponse)
async def get_html_report(job_id: str):
    """Generate HTML report for completed analysis"""
    try:
        job_data = redis_client.get(f"analysis_job:{job_id}")
        if not job_data:
            raise HTTPException(status_code=404, detail="Analysis job not found")
        
        job = json.loads(job_data)
        
        if job.get("status") != "completed":
            raise HTTPException(status_code=400, detail="Analysis not completed yet")
        
        if "result" not in job:
            raise HTTPException(status_code=400, detail="No analysis result found")

        result = job["result"]

        # Result should already be a dict from Redis
        # Check both framework and evaluation_type fields
        framework = result.get("framework", "")
        evaluation_type = result.get("evaluation_type", "")

        # Use diagnostic report generator for cbil_comprehensive framework
        if "cbil_comprehensive" in framework or "cbil_comprehensive" in evaluation_type:
            logger.info(f"Using Diagnostic report generator for cbil_comprehensive framework")
            logger.info(f"Result type: {type(result)}")
            logger.info(f"Result keys: {result.keys() if isinstance(result, dict) else 'NOT A DICT'}")
            try:
                html_report = diagnostic_report_generator.generate_html_report(result)
            except Exception as gen_error:
                import traceback
                logger.error(f"Diagnostic report generation failed: {str(gen_error)}")
                logger.error(f"Traceback: {traceback.format_exc()}")
                logger.error(f"Result structure: {json.dumps(result, indent=2, default=str)[:500]}")
                raise
        else:
            # Use standard HTML report generator for other frameworks
            html_report = report_generator.generate_html_report(result)

        return HTMLResponse(content=html_report, media_type="text/html")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating HTML report: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/api/reports/diagnostic/{job_id}", response_class=HTMLResponse)
async def get_diagnostic_report(job_id: str):
    """
    Generate diagnostic professional diagnostic report for completed analysis

    This endpoint provides a medical body composition-style report with:
    - At-a-glance hero summary with overall score
    - Core metric score cards (top 3 metrics)
    - Strengths and areas for improvement
    - Priority coaching recommendations
    - Professional Diagnostic-inspired design system

    Only works for comprehensive analysis (cbil_comprehensive framework)
    """
    try:
        job_data = redis_client.get(f"analysis_job:{job_id}")
        if not job_data:
            raise HTTPException(status_code=404, detail="Analysis job not found")

        job = json.loads(job_data)

        if job.get("status") != "completed":
            raise HTTPException(status_code=400, detail="Analysis not completed yet")

        if "result" not in job:
            raise HTTPException(status_code=400, detail="No analysis result found")

        result = job["result"]
        framework = result.get("framework", "")

        # Diagnostic reports are only available for comprehensive analysis
        if framework != "cbil_comprehensive":
            raise HTTPException(
                status_code=400,
                detail="Diagnostic reports are only available for comprehensive analysis (cbil_comprehensive framework)"
            )

        # Verify required data exists
        if "quantitative_metrics" not in result:
            raise HTTPException(
                status_code=400,
                detail="Quantitative metrics not found in analysis result"
            )

        # Generate diagnostic report
        logger.info(f"Generating Diagnostic report for job {job_id}")
        diagnostic_html = diagnostic_report_generator.generate_html_report(result)

        return HTMLResponse(content=diagnostic_html, media_type="text/html")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating Diagnostic report: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.get("/api/reports/data/{job_id}")
async def get_report_data(job_id: str):
    """Get structured report data for frontend"""
    try:
        job_data = redis_client.get(f"analysis_job:{job_id}")
        if not job_data:
            raise HTTPException(status_code=404, detail="Analysis job not found")
        
        job = json.loads(job_data)
        
        if job.get("status") != "completed":
            raise HTTPException(status_code=400, detail="Analysis not completed yet")
        
        if "result" not in job:
            raise HTTPException(status_code=400, detail="No analysis result found")
        
        result = job["result"]
        framework = result.get("framework", "generic")

        # Extract chart data and recommendations
        # For CBIL comprehensive, use coaching_feedback instead of raw analysis
        if framework == "cbil_comprehensive":
            # Use coaching feedback for rich recommendations
            coaching_data = result.get("coaching_feedback", {})
            analysis_text_for_extraction = result.get("cbil_analysis_text", "")
            recommendations_text = coaching_data.get("priority_actions", [])
        else:
            analysis_text_for_extraction = result.get("analysis", "")
            recommendations_text = analysis_text_for_extraction

        chart_data = report_generator.extract_chart_data(analysis_text_for_extraction, framework)
        recommendations = report_generator.generate_recommendations(recommendations_text, framework)
        
        return {
            "analysis_id": job_id,
            "framework": framework,
            "framework_name": result.get("framework_name", "ë¶„ì„"),
            "chart_data": chart_data,
            "chart_config": report_generator.generate_chart_js_config(chart_data),
            "recommendations": recommendations,
            "analysis_text": analysis_text_for_extraction,
            "coaching_feedback": result.get("coaching_feedback", {}) if framework == "cbil_comprehensive" else None,
            "metadata": {
                "character_count": result.get("character_count", 0),
                "word_count": result.get("word_count", 0),
                "created_at": result.get("created_at", "")
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting report data: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/api/reports/pdf/{job_id}")
async def get_pdf_report(job_id: str):
    """Generate and download PDF report for completed analysis"""
    try:
        # Check if PDF generation is available
        if not is_pdf_generation_available():
            raise HTTPException(
                status_code=503, 
                detail="PDF generation is not available. WeasyPrint is not installed."
            )
        
        job_data = redis_client.get(f"analysis_job:{job_id}")
        if not job_data:
            raise HTTPException(status_code=404, detail="Analysis job not found")
        
        job = json.loads(job_data)
        
        if job.get("status") != "completed":
            raise HTTPException(status_code=400, detail="Analysis not completed yet")
        
        if "result" not in job:
            raise HTTPException(status_code=400, detail="No analysis result found")
        
        # Generate PDF
        pdf_bytes = pdf_generator.generate_pdf_report(job["result"])
        
        # Generate filename
        filename = pdf_generator.generate_pdf_filename(job["result"])
        
        # Return PDF as download
        return Response(
            content=pdf_bytes,
            media_type="application/pdf",
            headers={"Content-Disposition": f"attachment; filename={filename}"}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating PDF report: {str(e)}")
        raise HTTPException(status_code=500, detail=f"PDF generation failed: {str(e)}")

@app.get("/api/reports/pdf-enhanced/{job_id}")
async def get_enhanced_pdf_report(job_id: str, include_cover: bool = True):
    """
    Generate enhanced PDF report with rendered charts (not placeholders)

    Uses AdvancedPDFGenerator with Matplotlib chart rendering
    """
    try:
        # Check if PDF generation is available
        if not is_pdf_generation_available():
            raise HTTPException(
                status_code=503,
                detail="PDF generation is not available. WeasyPrint is not installed."
            )

        job_data = redis_client.get(f"analysis_job:{job_id}")
        if not job_data:
            raise HTTPException(status_code=404, detail="Analysis job not found")

        job = json.loads(job_data)

        if job.get("status") != "completed":
            raise HTTPException(status_code=400, detail="Analysis not completed yet")

        if "result" not in job:
            raise HTTPException(status_code=400, detail="No analysis result found")

        result = job["result"]
        framework = result.get("framework", "generic")

        # Only generate enhanced PDFs for cbil_comprehensive framework
        if framework != "cbil_comprehensive":
            raise HTTPException(
                status_code=400,
                detail="Enhanced PDF only available for cbil_comprehensive framework"
            )

        # Generate enhanced PDF with rendered charts
        logger.info(f"Generating enhanced PDF for job {job_id}")
        pdf_bytes = advanced_pdf_gen.generate_pdf_with_charts(
            result,
            include_cover=include_cover
        )

        # Generate filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"CBIL_Enhanced_Report_{job_id[:8]}_{timestamp}.pdf"

        logger.info(f"Enhanced PDF generated successfully: {filename}")

        return Response(
            content=pdf_bytes,
            media_type="application/pdf",
            headers={"Content-Disposition": f"attachment; filename={filename}"}
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating enhanced PDF: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"Enhanced PDF generation failed: {str(e)}")

@app.get("/api/reports/visualization/3d-matrix/{job_id}", response_class=HTMLResponse)
async def get_3d_matrix_visualization(job_id: str):
    """
    Get interactive 3D matrix heatmap visualization

    Returns interactive Plotly 3D scatter plot showing Stage Ã— Context Ã— Level
    """
    try:
        job_data = redis_client.get(f"analysis_job:{job_id}")
        if not job_data:
            raise HTTPException(status_code=404, detail="Analysis job not found")

        job = json.loads(job_data)

        if job.get("status") != "completed":
            raise HTTPException(status_code=400, detail="Analysis not completed yet")

        if "result" not in job:
            raise HTTPException(status_code=400, detail="No analysis result found")

        result = job["result"]
        framework = result.get("framework", "generic")

        # Only available for cbil_comprehensive framework
        if framework != "cbil_comprehensive":
            raise HTTPException(
                status_code=400,
                detail="3D matrix visualization only available for cbil_comprehensive framework"
            )

        # Extract matrix data from Module 2 results
        module2_result = result.get("module2_result")
        if not module2_result:
            raise HTTPException(
                status_code=400,
                detail="No Module 2 matrix data found in analysis result"
            )

        # Generate 3D visualization
        logger.info(f"Generating 3D matrix visualization for job {job_id}")
        html_content = matrix_3d_viz.generate_3d_heatmap(module2_result)

        logger.info(f"3D visualization generated successfully")

        return HTMLResponse(content=html_content, media_type="text/html")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating 3D visualization: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"3D visualization failed: {str(e)}")

@app.get("/api/reports/excel/{job_id}")
async def get_excel_report(job_id: str):
    """
    Generate Excel workbook with comprehensive analysis data

    Creates multi-sheet Excel file with:
    - Executive Summary
    - CBIL Scores
    - Module 3 Metrics
    - 3D Matrix Data
    - Pattern Matching
    - Coaching Feedback
    """
    try:
        job_data = redis_client.get(f"analysis_job:{job_id}")
        if not job_data:
            raise HTTPException(status_code=404, detail="Analysis job not found")

        job = json.loads(job_data)

        if job.get("status") != "completed":
            raise HTTPException(status_code=400, detail="Analysis not completed yet")

        if "result" not in job:
            raise HTTPException(status_code=400, detail="No analysis result found")

        result = job["result"]
        framework = result.get("framework", "generic")

        # Generate Excel export
        logger.info(f"Generating Excel export for job {job_id}, framework: {framework}")
        excel_bytes = excel_exporter.export_to_excel(result)

        # Generate filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        framework_name = framework.replace("_", "-")
        filename = f"Analysis_Report_{framework_name}_{job_id[:8]}_{timestamp}.xlsx"

        logger.info(f"Excel export generated successfully: {filename}")

        return Response(
            content=excel_bytes,
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={"Content-Disposition": f"attachment; filename={filename}"}
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating Excel export: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"Excel export failed: {str(e)}")

@app.get("/api/reports/visualization/2d-heatmaps/{job_id}", response_class=HTMLResponse)
async def get_2d_heatmaps(job_id: str):
    """
    Get 2D heatmap slices by cognitive level

    Returns three 2D heatmaps showing Stage Ã— Context for each Level (L1, L2, L3)
    """
    try:
        job_data = redis_client.get(f"analysis_job:{job_id}")
        if not job_data:
            raise HTTPException(status_code=404, detail="Analysis job not found")

        job = json.loads(job_data)

        if job.get("status") != "completed":
            raise HTTPException(status_code=400, detail="Analysis not completed yet")

        if "result" not in job:
            raise HTTPException(status_code=400, detail="No analysis result found")

        result = job["result"]
        framework = result.get("framework", "generic")

        # Only available for cbil_comprehensive framework
        if framework != "cbil_comprehensive":
            raise HTTPException(
                status_code=400,
                detail="2D heatmaps only available for cbil_comprehensive framework"
            )

        # Extract matrix data from Module 2 results
        module2_result = result.get("module2_result")
        if not module2_result:
            raise HTTPException(
                status_code=400,
                detail="No Module 2 matrix data found in analysis result"
            )

        # Generate 2D heatmaps
        logger.info(f"Generating 2D heatmap slices for job {job_id}")
        html_content = matrix_3d_viz.generate_2d_heatmaps(module2_result)

        logger.info(f"2D heatmaps generated successfully")

        return HTMLResponse(content=html_content, media_type="text/html")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating 2D heatmaps: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"2D heatmap generation failed: {str(e)}")

@app.get("/api/reports/visualization/distributions/{job_id}", response_class=HTMLResponse)
async def get_distribution_charts(job_id: str):
    """
    Get distribution bar charts for Stage, Context, and Level

    Returns three bar charts showing percentage distributions across each dimension
    """
    try:
        job_data = redis_client.get(f"analysis_job:{job_id}")
        if not job_data:
            raise HTTPException(status_code=404, detail="Analysis job not found")

        job = json.loads(job_data)

        if job.get("status") != "completed":
            raise HTTPException(status_code=400, detail="Analysis not completed yet")

        if "result" not in job:
            raise HTTPException(status_code=400, detail="No analysis result found")

        result = job["result"]
        framework = result.get("framework", "generic")

        # Only available for cbil_comprehensive framework
        if framework != "cbil_comprehensive":
            raise HTTPException(
                status_code=400,
                detail="Distribution charts only available for cbil_comprehensive framework"
            )

        # Extract matrix data from Module 2 results
        module2_result = result.get("module2_result")
        if not module2_result:
            raise HTTPException(
                status_code=400,
                detail="No Module 2 matrix data found in analysis result"
            )

        # Generate distribution charts
        logger.info(f"Generating distribution charts for job {job_id}")
        html_content = matrix_3d_viz.generate_distribution_charts(module2_result)

        logger.info(f"Distribution charts generated successfully")

        return HTMLResponse(content=html_content, media_type="text/html")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating distribution charts: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"Distribution chart generation failed: {str(e)}")

# Report Generation Request Models
class ReportGenerationRequest(BaseModel):
    analysis_result: Dict[str, Any]
    template: str = "comprehensive"
    title: Optional[str] = None

class ComprehensiveReportRequest(BaseModel):
    analyses: List[Dict[str, Any]]
    configuration: Optional[Dict[str, Any]] = {}
    
    @validator('analyses')
    def validate_analyses(cls, v):
        if not v or len(v) < 1:
            raise ValueError('At least one analysis is required')
        if len(v) > 10:
            raise ValueError('Maximum 10 analyses can be combined')
        return v

@app.post("/api/reports/generate/html", response_class=HTMLResponse)
async def generate_html_report(request: ReportGenerationRequest):
    """Generate HTML report from analysis data"""
    try:
        # Extract analysis data
        analysis_result = request.analysis_result
        
        # Prepare data for HTML generator
        report_data = {
            'framework': analysis_result.get('framework', 'generic'),
            'analysis': analysis_result.get('analysis', ''),
            'analysis_id': analysis_result.get('analysis_id', 'direct-generation'),
            'created_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'character_count': analysis_result.get('character_count', 0),
            'word_count': analysis_result.get('word_count', 0),
            'metadata': analysis_result.get('metadata', {})
        }
        
        # Generate HTML report
        html_content = report_generator.generate_html_report(report_data)
        
        logger.info(f"Generated HTML report for framework: {report_data['framework']}")
        
        return HTMLResponse(
            content=html_content,
            headers={
                "Content-Type": "text/html; charset=utf-8",
                "Cache-Control": "no-cache"
            }
        )
        
    except Exception as e:
        logger.error(f"Error generating HTML report: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Report generation failed: {str(e)}")

@app.get("/api/reports/status/{job_id}")
async def get_report_status(job_id: str):
    """Get analysis job status for report generation"""
    try:
        job_data = redis_client.get(f"analysis_job:{job_id}")
        if not job_data:
            raise HTTPException(status_code=404, detail="Analysis job not found")
        
        return json.loads(job_data)
    
    except Exception as e:
        logger.error(f"Error getting report status: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/api/reports/status")
async def get_report_capabilities():
    """Get report generation capabilities"""
    return {
        "html_available": True,
        "pdf_available": is_pdf_generation_available(),
        "supported_formats": ["html", "pdf"] if is_pdf_generation_available() else ["html"],
        "frameworks_supported": list(report_generator.FRAMEWORK_NAMES.keys())
    }

@app.get("/api/stats")
async def get_stats():
    """Get service statistics"""
    try:
        # Get all job keys
        job_keys = redis_client.keys("analysis_job:*")
        
        stats = {
            "total_analyses": len(job_keys),
            "service": "analysis",
            "timestamp": datetime.now().isoformat(),
            "available_frameworks": len(ANALYSIS_FRAMEWORKS)
        }
        
        if job_keys:
            # Count by status and framework
            status_counts = {}
            framework_counts = {}
            
            for key in job_keys:
                job_data = json.loads(redis_client.get(key))
                status = job_data.get("status", "unknown")
                framework = job_data.get("framework", "unknown")
                
                status_counts[status] = status_counts.get(status, 0) + 1
                framework_counts[framework] = framework_counts.get(framework, 0) + 1
            
            stats["status_breakdown"] = status_counts
            stats["framework_breakdown"] = framework_counts
        
        return stats
    
    except Exception as e:
        logger.error(f"Error getting stats: {str(e)}")
        return {"error": "Could not retrieve stats"}

@app.post("/api/reports/generate/comprehensive", response_class=HTMLResponse)
async def generate_comprehensive_report(request: ComprehensiveReportRequest):
    """Generate comprehensive HTML report combining multiple analysis results"""
    try:
        # Extract analysis results directly from the request
        analysis_results = request.analyses
        
        if not analysis_results:
            raise HTTPException(
                status_code=400,
                detail="No analysis results provided"
            )
        
        # Extract configuration
        config = request.configuration or {}
        
        # Prepare report configuration with defaults
        report_config = {
            "report_type": config.get("type", "detailed"),
            "framework_weights": config.get("frameworkWeights", {}),
            "include_recommendations": config.get("includeRecommendations", True),
            "title": config.get("title", "ì¢…í•© êµìœ¡ ë¶„ì„ ë³´ê³ ì„œ")
        }
        
        logger.info(f"Generating comprehensive report with {len(analysis_results)} analyses")
        logger.info(f"Report config: {report_config}")
        
        # Generate comprehensive report
        html_report = report_generator.generate_comprehensive_report(
            analysis_results, 
            report_config
        )
        
        logger.info(f"Generated comprehensive report combining {len(analysis_results)} analyses")
        
        return HTMLResponse(
            content=html_report,
            headers={
                "Content-Type": "text/html; charset=utf-8",
                "Cache-Control": "no-cache",
                "X-Analysis-Count": str(len(analysis_results))
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating comprehensive report: {str(e)}")
        raise HTTPException(
            status_code=500, 
            detail=f"Comprehensive report generation failed: {str(e)}"
        )

@app.get("/api/reports/comprehensive/status/{job_ids}")
async def get_comprehensive_report_status(job_ids: str):
    """Get status of multiple analysis jobs for comprehensive reporting"""
    try:
        # Parse comma-separated job IDs
        job_id_list = [id.strip() for id in job_ids.split(',') if id.strip()]
        
        if len(job_id_list) > 10:
            raise HTTPException(
                status_code=400, 
                detail="Maximum 10 job IDs can be checked at once"
            )
        
        job_statuses = []
        summary = {
            "total_jobs": len(job_id_list),
            "completed": 0,
            "processing": 0,
            "pending": 0,
            "failed": 0,
            "missing": 0,
            "ready_for_comprehensive": False
        }
        
        for job_id in job_id_list:
            try:
                job_data = redis_client.get(f"analysis_job:{job_id}")
                if not job_data:
                    job_statuses.append({
                        "job_id": job_id,
                        "status": "missing",
                        "message": "Job not found"
                    })
                    summary["missing"] += 1
                    continue
                
                job = json.loads(job_data)
                status = job.get("status", "unknown")
                
                job_status = {
                    "job_id": job_id,
                    "status": status,
                    "framework": job.get("framework"),
                    "message": job.get("message", ""),
                    "has_result": "result" in job,
                    "created_at": job.get("created_at"),
                    "updated_at": job.get("updated_at")
                }
                
                if status == "completed" and "result" in job:
                    framework_name = report_generator.FRAMEWORK_NAMES.get(
                        job.get("framework"), 
                        job.get("framework", "Unknown")
                    )
                    job_status["framework_name"] = framework_name
                    summary["completed"] += 1
                elif status == "processing":
                    summary["processing"] += 1
                elif status == "pending":
                    summary["pending"] += 1
                elif status == "failed":
                    summary["failed"] += 1
                
                job_statuses.append(job_status)
                
            except json.JSONDecodeError:
                job_statuses.append({
                    "job_id": job_id,
                    "status": "data_error",
                    "message": "Invalid job data format"
                })
                summary["failed"] += 1
            except Exception as e:
                logger.error(f"Error checking status of job {job_id}: {str(e)}")
                job_statuses.append({
                    "job_id": job_id,
                    "status": "error",
                    "message": str(e)
                })
                summary["failed"] += 1
        
        # Check if ready for comprehensive report
        summary["ready_for_comprehensive"] = (
            summary["completed"] > 0 and 
            summary["processing"] == 0 and 
            summary["pending"] == 0
        )
        
        return {
            "job_statuses": job_statuses,
            "summary": summary,
            "timestamp": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error checking comprehensive report status: {str(e)}")
        raise HTTPException(status_code=500, detail="Error checking job statuses")

@app.get("/api/research/stats")
async def get_research_stats():
    """Get research and database statistics"""
    try:
        db = next(get_db())
        research_stats = get_research_statistics(db)
        db.close()
        
        # Add Redis stats
        redis_keys = redis_client.keys("analysis_job:*")
        research_stats["redis_jobs"] = len(redis_keys)
        research_stats["timestamp"] = datetime.now().isoformat()
        
        return research_stats
        
    except Exception as e:
        logger.error(f"Error getting research stats: {str(e)}")
        return {"error": "Could not retrieve research statistics"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)